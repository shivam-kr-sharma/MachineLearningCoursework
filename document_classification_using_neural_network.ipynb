{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More about this task can be found on the link below:\n",
    "https://www.kaggle.com/competitions/idc-410-document-classification?rvi=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Training data shape: (960,)\n",
      "Validation data shape: (240,)\n",
      "Test data shape: (300, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate the features and labels in the training data\n",
    "train_text = train_data['text']\n",
    "train_labels = train_data['label']\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(train_text, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the training, validation, and test sets\n",
    "print('Training data shape:', train_text.shape)\n",
    "print('Validation data shape:', val_text.shape)\n",
    "print('Test data shape:', test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed training data shape: (960, 2364)\n",
      "Preprocessed validation data shape: (240, 2364)\n",
      "Preprocessed test data shape: (300, 2364)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer with the maximum number of words to keep\n",
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Convert the text data to sequences of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "# Get the maximum sequence length in the training data\n",
    "MAX_SEQUENCE_LENGTH = max(len(seq) for seq in train_sequences)\n",
    "\n",
    "# Pad the sequences to the maximum length\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_data = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Print the shape of the preprocessed training, validation, and test data\n",
    "print('Preprocessed training data shape:', train_data.shape)\n",
    "print('Preprocessed validation data shape:', val_data.shape)\n",
    "print('Preprocessed test data shape:', test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "34/34 [==============================] - 3s 53ms/step - loss: 1.5999 - accuracy: 0.2907 - val_loss: 1.5842 - val_accuracy: 0.2917\n",
      "Epoch 2/35\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.4130 - accuracy: 0.7630 - val_loss: 1.3548 - val_accuracy: 0.6750\n",
      "Epoch 3/35\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 0.8381 - accuracy: 0.8963 - val_loss: 0.7425 - val_accuracy: 0.8333\n",
      "Epoch 4/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.2321 - accuracy: 0.9722 - val_loss: 0.5325 - val_accuracy: 0.8583\n",
      "Epoch 5/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0560 - accuracy: 0.9907 - val_loss: 0.4885 - val_accuracy: 0.8750\n",
      "Epoch 6/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0280 - accuracy: 0.9963 - val_loss: 0.4726 - val_accuracy: 0.9083\n",
      "Epoch 7/35\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 0.0236 - accuracy: 0.9954 - val_loss: 0.4960 - val_accuracy: 0.9000\n",
      "Epoch 8/35\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 0.0210 - accuracy: 0.9972 - val_loss: 0.4788 - val_accuracy: 0.8917\n",
      "Epoch 9/35\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 0.0215 - accuracy: 0.9963 - val_loss: 0.5186 - val_accuracy: 0.9000\n",
      "Epoch 10/35\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 0.0218 - accuracy: 0.9944 - val_loss: 0.5025 - val_accuracy: 0.9083\n",
      "Epoch 11/35\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.5060 - val_accuracy: 0.8667\n",
      "Epoch 12/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0169 - accuracy: 0.9963 - val_loss: 0.5653 - val_accuracy: 0.8917\n",
      "Epoch 13/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0227 - accuracy: 0.9963 - val_loss: 0.4903 - val_accuracy: 0.9000\n",
      "Epoch 14/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0208 - accuracy: 0.9963 - val_loss: 0.4973 - val_accuracy: 0.9083\n",
      "Epoch 15/35\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 0.0152 - accuracy: 0.9963 - val_loss: 0.5308 - val_accuracy: 0.9083\n",
      "Epoch 16/35\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 0.0163 - accuracy: 0.9963 - val_loss: 0.5053 - val_accuracy: 0.9000\n",
      "Epoch 17/35\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 0.0149 - accuracy: 0.9972 - val_loss: 0.5749 - val_accuracy: 0.8750\n",
      "Epoch 18/35\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 0.0193 - accuracy: 0.9944 - val_loss: 0.5353 - val_accuracy: 0.9167\n",
      "Epoch 19/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.5278 - val_accuracy: 0.9167\n",
      "Epoch 20/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0166 - accuracy: 0.9963 - val_loss: 0.5665 - val_accuracy: 0.9000\n",
      "Epoch 21/35\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.5530 - val_accuracy: 0.8750\n",
      "Epoch 22/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0183 - accuracy: 0.9963 - val_loss: 0.5774 - val_accuracy: 0.9000\n",
      "Epoch 23/35\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.5458 - val_accuracy: 0.9167\n",
      "Epoch 24/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.5468 - val_accuracy: 0.9167\n",
      "Epoch 25/35\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.5765 - val_accuracy: 0.9083\n",
      "Epoch 26/35\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 0.0154 - accuracy: 0.9944 - val_loss: 0.5544 - val_accuracy: 0.8917\n",
      "Epoch 27/35\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 0.0141 - accuracy: 0.9972 - val_loss: 0.6246 - val_accuracy: 0.8917\n",
      "Epoch 28/35\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.5844 - val_accuracy: 0.9000\n",
      "Epoch 29/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.6121 - val_accuracy: 0.8917\n",
      "Epoch 30/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.5629 - val_accuracy: 0.9083\n",
      "Epoch 31/35\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.5937 - val_accuracy: 0.9083\n",
      "Epoch 32/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.6377 - val_accuracy: 0.8917\n",
      "Epoch 33/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0155 - accuracy: 0.9963 - val_loss: 0.5941 - val_accuracy: 0.8750\n",
      "Epoch 34/35\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.6046 - val_accuracy: 0.9083\n",
      "Epoch 35/35\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.5938 - val_accuracy: 0.8917\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5938 - accuracy: 0.8917\n",
      "Validation accuracy: 0.8916666507720947\n",
      "10/10 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the train and test data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Tokenize the text data\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df['text'].values)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'].values)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'].values)\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Convert the label data to integers\n",
    "label_to_int = {'sport': 0, 'business': 1, 'politics': 2, 'tech': 3, 'entertainment': 4}\n",
    "train_df['label'] = train_df['label'].map(label_to_int)\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "val_size = 0.1\n",
    "val_samples = int(len(train_data) * val_size)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=val_size)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(MAX_NUM_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),  \n",
    "      \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 35\n",
    "BATCH_SIZE = 32\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_data, val_labels))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_acc = model.evaluate(val_data, val_labels)\n",
    "print('Validation accuracy:', val_acc)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(test_data)\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Map the predicted labels back to their text values\n",
    "int_to_label = {0: 'sport', 1: 'business', 2: 'politics', 3: 'tech', 4: 'entertainment'}\n",
    "test_predictions = [int_to_label[i] for i in test_predictions]\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_df = pd.DataFrame({'index': test_df['index'], 'label': test_predictions})\n",
    "output_df.to_csv('test_predictions15.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
