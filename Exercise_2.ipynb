{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Programming Exercise 2\n",
    "\n",
    "## 1 Write a function to generate an m+1 dimensional data set, of size n, consisting of m continuous independent variables (X) and one dependent binary variable (Y) defined as\n",
    "\n",
    "$Y = \\begin{cases} \n",
    "      1 & \\text{if } p(y = 1|\\mathbf{x}) = \\frac{1}{1 + \\exp(-\\mathbf{x}^\\top\\mathbf{\\beta})} > 0.5\\\\\n",
    "      0 & \\text{otherwise}\n",
    "   \\end{cases}$\n",
    "\n",
    "where,  \n",
    "\n",
    "- β is a random vector of dimensionality m + 1, representing the coefficients of the linear relationship between X and Y, and\n",
    "- $\\forall i \\in [1, n], x_{i0} = 1$\n",
    "\n",
    "To add noise to the labels (Y) generated, we assume a Bernoulli distribution with probability of success, θ, that determines whether or not the label generated, as above, is to be flipped. The larger the value of θ, the greater is the noise.\n",
    "\n",
    "The function should take the following parameters:  \n",
    "\n",
    "- θ: The probability of flipping the label, Y  \n",
    "- n: The size of the data set\n",
    "- m: The number of indepedent variables  \n",
    "\n",
    "Output from the function should be:  \n",
    "\n",
    "- X: An n × m numpy array of independent variable values (with a 1 in the first column)\n",
    "- Y: The n × 1 binary numpy array of output values  \n",
    "- β: The random coefficients used to generate Y from X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(theta, datasetSize, m):                 # Here theta is the probability of flipping the label Y\n",
    "    beta = np.random.rand(m+1)\n",
    "    X = np.random.rand(datasetSize, m) \n",
    "    X = np.hstack((np.ones((datasetSize, 1)), X)) \n",
    "    h = 1 / (1 + np.exp(-X @ beta)) \n",
    "    Y=np.zeros(datasetSize) \n",
    "    for i in range (0,datasetSize,1) : \n",
    "      if h[i]>0.5 :\n",
    "        Y[i]=Y[i]+1\n",
    "    noise = np.random.binomial(1, theta, datasetSize)\n",
    "    #This will switch values of Y if noise is 1\n",
    "    for i in range(0,datasetSize,1) :\n",
    "      if noise[i]==1 and Y[i]==1 :\n",
    "        Y[i] -= 1\n",
    "      elif noise[i]==1 and Y[i]==0 :\n",
    "        Y[i] += 1\n",
    "      else :\n",
    "        Y[i]=Y[i]+0\n",
    "    Y = Y.reshape(datasetSize,1)\n",
    "    return X, Y, beta\n",
    "\n",
    "generate_data(0.5, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Write a function that learns the parameters of a logistic regression function given inputs  \n",
    "\n",
    "- X: An n × m numpy array of independent variable values  \n",
    "- Y: The n × 1 binary numpy array of output values\n",
    "- k: the number of iteractions (epochs)  \n",
    "- τ: the threshold on change in Cost function value from the previous to current iteration \n",
    "- λ: the learning rate for Gradient Descent  \n",
    "\n",
    "The function should implement the Gradient Descent algorithm as discussed in class that initialises β with random values and then updates these values in each iteraction by moving in the the direction defined by the partial derivative of the cost function with respect to each of the coefficients. The function should use only one loop that ends after a number of iterations (k) or a threshold on the change in cost function value (τ).\n",
    "\n",
    "The output should be a m + 1 dimensional vector of coefficients and the final cost function value.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_regression(X, Y, k, tau, lamda):    # Here tau is the threshold on change in Cost function value from the previous to current iteration. And, lamda is the learning rate for gradient descent.\n",
    " \n",
    "    n, m = X.shape\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    beta = np.random.rand(m + 1)\n",
    "    cost = float(\"inf\")\n",
    "    \n",
    "    for i in range(k):\n",
    "        Z = np.dot(X, beta)\n",
    "        H = 1/(1 + np.exp(-Z))\n",
    "        error = H - Y\n",
    "        gradient = np.dot(X.T, error) / n\n",
    "\n",
    "        cost_new = -np.mean(Y * np.log(H) + (1 - Y) * np.log(1 - H))\n",
    "        if abs(cost - cost_new) < tau:\n",
    "            break\n",
    "        cost = cost_new\n",
    "        beta -= lamda * gradient\n",
    "        \n",
    "    return beta, cost\n",
    "\n",
    "y = np.arange(3)\n",
    "x = np.arange(15)\n",
    "x = x.reshape(3,5)\n",
    "k=2000\n",
    "t =0.3\n",
    "l=0.01\n",
    "logistic_regression(x, y, k, t, l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
