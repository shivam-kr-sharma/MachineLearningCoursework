{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#   Frame Level Speech Recognition with Neural Networks\n",
    "#################################\n",
    "\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadRaw(path, name):\n",
    "    return (\n",
    "        np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes', allow_pickle=True), \n",
    "        np.load(os.path.join(path, '{}_labels.npy'.format(name)), encoding='bytes', allow_pickle=True)\n",
    "    )\n",
    "# Load Data\n",
    "temp_X, temp_Y = loadRaw(\"C:/Users/LENOVO/Downloads/1603866758-5e748a2d5fc288e9f69c5f86 (3)/d611ef4f2eccddb5581e0ac617ce38eb-fa231bbdb1390a35acb20526d7302f26b5f30ea2\", 'dev')\n",
    "\n",
    "#   One Frame - One Phonem Model.\n",
    "# In this model, we assume a 1 frame to 1 phoneme mapping. \n",
    "# This means that the input layer has a length of 40, while the output layer has a length of 138. \n",
    "# To handle the categorical labels of the data, we use Sparse Categorical Cross entropy as the loss function and the ADAM optimizer. \n",
    "# The activation function for all layers, except the last layer, is ReLU, while the last layer uses Softmax.\n",
    "# We experimented with the number of layers and found that there was only a slight improvement in test accuracy between 2 and 3 layers. \n",
    "# However, to be cautious, we chose to use 4 layers. \n",
    "# The number of nodes in each layer was chosen randomly, but this can also be optimized.\n",
    "\n",
    "\n",
    "# Concat and shuffle all frames\n",
    "train_X = np.concatenate(temp_X)\n",
    "train_Y = np.concatenate(temp_Y)\n",
    "Arguments = np.arange(len(train_X))\n",
    "np.random.shuffle(Arguments)\n",
    "train_X = train_X[Arguments]\n",
    "train_Y = train_Y[Arguments]\n",
    "modelA = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(90, activation='relu'),\n",
    "    tf.keras.layers.Dense(270, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(138, activation='softmax'),\n",
    "])\n",
    "lossFunction = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "modelA.compile(optimizer='adam',\n",
    "              loss=lossFunction,\n",
    "              metrics=['accuracy'])\n",
    "modelA.evaluate(train_X[10000:25000], train_Y[10000:25000])\n",
    "\n",
    "historyA = modelA.fit(train_X[:100000], train_Y[:100000], epochs=50, verbose=2)\n",
    "\n",
    "\n",
    "plt.plot(historyA.history['accuracy'])\n",
    "plt.title(\"Naive model, accuracy vs epoch\")\n",
    "plt.show()\n",
    "\n",
    "modelA.evaluate(train_X[10000:25000], train_Y[10000:25000])\n",
    "\n",
    "\n",
    "\n",
    "#   Context Based Model\n",
    "# In this model, we consider adjacent frames to contain information relevant to the phoneme being predicted. \n",
    "# Since most of the sounds we produce last longer than a single frame of 10ms, we combine i-1, i, and i+1 frames to form a list of 120 elements, \n",
    "# where the ith phoneme is the corresponding label. \n",
    "# Due to the categorical nature of data labels, we use SparseCategoricalCrossentropy as the loss function and ADAM optimizer, \n",
    "# which is commonly used in similar models. \n",
    "# The activation function for all layers except the last one is relu, while softmax is used for the final layer.To determine the optimal number of layers, we increased the number of layers and checked whether the test accuracy increased. \n",
    "# We found that there was only a slight improvement between 2 and 3 layers, so we chose 4 layers to err on the side of caution. \n",
    "# The number of nodes in each layer was randomly selected but can be optimized as well.\n",
    "\n",
    "\n",
    "trainX_wc = []\n",
    "trainY_wc = []\n",
    "for datapoint in range(len(temp_X)):\n",
    "    for frame in range(1, len(temp_X[datapoint])-2):\n",
    "        # for each frame in each datapoint, concat neighbouring frames\n",
    "        trainX_wc.append(np.concatenate(temp_X[datapoint][frame-1:frame+2]))\n",
    "        trainY_wc.append(temp_Y[datapoint][frame])\n",
    "trainX_wc = np.array(trainX_wc)\n",
    "trainY_wc = np.array(trainY_wc)\n",
    "Arguments = np.arange(len(trainX_wc))\n",
    "np.random.shuffle(Arguments)\n",
    "trainX_wc = trainX_wc[Arguments]\n",
    "trainY_wc = trainY_wc[Arguments]\n",
    "modelB = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(180, activation='relu'),\n",
    "    tf.keras.layers.Dense(270, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(138, activation='softmax'),\n",
    "])\n",
    "\n",
    "lossFunction = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "modelB.compile(optimizer='adam',\n",
    "              loss=lossFunction,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelB.evaluate(trainX_wc[10000:25000], trainY_wc[10000:25000])\n",
    "\n",
    "historyB = modelB.fit(trainX_wc[:100000], trainY_wc[:100000], epochs=50, verbose=2)\n",
    "\n",
    "plt.plot(historyB.history['accuracy'])\n",
    "plt.title(\"Context based model, accuracy vs epoch\")\n",
    "plt.show()\n",
    "\n",
    "modelB.evaluate(trainX_wc[25000:75000], trainY_wc[25000:75000])\n",
    "\n",
    "\n",
    "\n",
    "###   The Naive model yielded an accuracy of 32.65%, while the Context-based model achieved 42.36%. \n",
    "# Additionally, the graph shows that the Context-based model trains to the maximum value more rapidly than the Naive model. \n",
    "# This indicates that using the Context-based model has a significant advantage over the Naive model. \n",
    "# To determine the optimal amount of context, one can run the code with a higher number of context frames and identify the point of maximum accuracy. \n",
    "# It is essential to note that selecting an overly extended segment may result in phoneme overlaps, leading to a decrease in accuracy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
