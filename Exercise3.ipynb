{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (45000, 784)\n",
      "X_test shape: (15000, 784)\n",
      "y_train shape: (45000,)\n",
      "y_test shape: (15000,)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "#### Splitting the data into test and train datasets\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_data = pd.read_csv(\"mnist_train.csv\")\n",
    "\n",
    "# Assuming the labels are in the first column and pixel values start from the second column\n",
    "X = mnist_data.iloc[:, 1:]\n",
    "Y = mnist_data.iloc[:, 0]\n",
    "# print(X)\n",
    "# print(X.shape)\n",
    "# print(Y)\n",
    "# print(Y.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# By default, the test_size is set to 0.25, meaning 25% of the data will be used for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", Y_train.shape)\n",
    "print(\"y_test shape:\", Y_test.shape)\n",
    "# print(\"X_test is : \" ,X_test)\n",
    "# print(X_test.shape)\n",
    "# print(\"Y_test is : \" , Y_test)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Cross-Entropy Loss: 2.3115208110030423\n",
      "Epoch 100/1000, Cross-Entropy Loss: 0.3771888817315826\n",
      "Epoch 200/1000, Cross-Entropy Loss: 0.33784919031490723\n",
      "Epoch 300/1000, Cross-Entropy Loss: 0.32015421176649206\n",
      "Epoch 400/1000, Cross-Entropy Loss: 0.30941356213108134\n",
      "Epoch 500/1000, Cross-Entropy Loss: 0.3019431035421183\n",
      "Epoch 600/1000, Cross-Entropy Loss: 0.29631796140716743\n",
      "Epoch 700/1000, Cross-Entropy Loss: 0.2918562423514657\n",
      "Epoch 800/1000, Cross-Entropy Loss: 0.28818574308202044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     logits \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(inputs, weights) \u001b[38;5;241m+\u001b[39m bias\n\u001b[1;32m---> 22\u001b[0m     exp_logits \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     softmax_output \u001b[38;5;241m=\u001b[39m exp_logits \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Cross-entropy loss\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(230499)\n",
    "no_of_rows, no_of_cols = X_train.shape\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights = np.zeros([no_of_cols, 10])  # Assuming 10 classes\n",
    "bias = np.random.rand(1, 10)\n",
    "\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "target_one_hot = np.eye(10)[Y_train]\n",
    "\n",
    "# Forward Pass\n",
    "inputs = np.array(X_train)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass\n",
    "    logits = np.dot(inputs, weights) + bias\n",
    "    exp_logits = np.exp(logits)\n",
    "    softmax_output = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = -np.sum(target_one_hot * np.log(softmax_output + 1e-8)) / no_of_rows\n",
    "    \n",
    "    # Optional: Print the loss for monitoring convergence\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Cross-Entropy Loss: {loss}\")\n",
    "\n",
    "    # Backward Pass\n",
    "    d_logits = softmax_output - target_one_hot\n",
    "    d_weights = np.dot(inputs.T, d_logits) / no_of_rows\n",
    "    d_bias = np.sum(d_logits, axis=0, keepdims=True) / no_of_rows\n",
    "\n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * d_weights\n",
    "    bias -= learning_rate * d_bias\n",
    "\n",
    "# After training, 'weights' and 'bias' will be adjusted for classification.\n",
    "print(\"Trained Weights shape:\", weights.shape)\n",
    "print(\"Trained Bias shape:\", bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58692288 0.68806999 0.67215694 ... 0.57511545 0.59606365 0.62362052]\n",
      " [0.58692288 0.68806999 0.67215694 ... 0.57511545 0.59606365 0.62362052]\n",
      " [0.58692288 0.68806999 0.67215694 ... 0.57511545 0.59606365 0.62362052]\n",
      " ...\n",
      " [0.58692288 0.68806999 0.67215694 ... 0.57511545 0.59606365 0.62362052]\n",
      " [0.58692288 0.68806999 0.67215694 ... 0.57511545 0.59606365 0.62362052]\n",
      " [0.58692288 0.68806999 0.67215694 ... 0.57511545 0.59606365 0.62362052]]\n",
      "(45000, 10)\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(230499)\n",
    "no_of_rows, no_of_cols = X_train.shape\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights = np.zeros([no_of_cols, 10])  # Assuming 10 classes\n",
    "bias = np.random.rand(1, 10)\n",
    "\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 5000\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "target_one_hot = np.eye(10)[Y_train]\n",
    "\n",
    "# Forward Pass\n",
    "inputs = np.array(X_train)\n",
    "\n",
    "logits = np.dot(inputs, weights) + bias\n",
    "\n",
    "exp_logits = np.exp(-logits)\n",
    "y = 1/(1 + exp_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 1)\n",
      "(15000, 1)\n",
      "Accuracy on Test Set: 92.24%\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass on Test Set\n",
    "inputs_test = np.array(X_test)\n",
    "logits_test = np.dot(inputs_test, weights) + bias\n",
    "softmax_output_test = np.exp(logits_test) / np.sum(np.exp(logits_test), axis=1, keepdims=True)\n",
    "\n",
    "# Predicted labels (argmax to get the class with the highest probability)\n",
    "predicted_labels = np.argmax(softmax_output_test, axis=1)\n",
    "predicted_labels = predicted_labels.reshape(-1, 1)\n",
    "print(predicted_labels.shape)\n",
    "Y_test = np.array(Y_test).reshape(-1, 1)\n",
    "print(Y_test.shape)\n",
    "# Accuracy calculation\n",
    "accuracy = np.mean(predicted_labels == Y_test)\n",
    "print(f\"Accuracy on Test Set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 9\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'weights' and 'bias' are the trained parameters\n",
    "\n",
    "def predict_class(input_vector):\n",
    "    input_vector = np.array(input_vector).reshape(1, -1)\n",
    "    logits = np.dot(input_vector, weights) + bias\n",
    "    softmax_output = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    predicted_class = np.argmax(softmax_output, axis=1)[0]\n",
    "    return predicted_class\n",
    "X_test = np.array(X_test)\n",
    "# # Example usage\n",
    "input_example = X_test[1005]  # Replace with the input vector you want to predict\n",
    "predicted_class_example = predict_class(input_example)\n",
    "print(f\"Predicted Class: {predicted_class_example}\")\n",
    "print(Y_test[1005])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_data = pd.read_csv(\"mnist_train.csv\")\n",
    "\n",
    "# Assuming the labels are in the first column and pixel values start from the second column\n",
    "X = mnist_data.iloc[:, 1:]\n",
    "Y = mnist_data.iloc[:, 0]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# By default, the test_size is set to 0.25, meaning 25% of the data will be used for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25020\\592973223.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  hidden_layer_output = 1 / (1 + np.exp(-(np.dot(inputs, weights_hidden) + bias_hidden)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Cross-Entropy Loss: 2.3372976045009226\n",
      "Epoch 100/1000, Cross-Entropy Loss: 1.305240538308685\n",
      "Epoch 200/1000, Cross-Entropy Loss: 0.9851093945331099\n",
      "Epoch 300/1000, Cross-Entropy Loss: 0.855250681962822\n",
      "Epoch 400/1000, Cross-Entropy Loss: 0.7754462981821066\n",
      "Epoch 500/1000, Cross-Entropy Loss: 0.7168516966258359\n",
      "Epoch 600/1000, Cross-Entropy Loss: 0.6772684746085585\n",
      "Epoch 700/1000, Cross-Entropy Loss: 0.6476989241909253\n",
      "Epoch 800/1000, Cross-Entropy Loss: 0.627892051697263\n",
      "Epoch 900/1000, Cross-Entropy Loss: 0.608518167367975\n",
      "Trained Weights (Hidden Layer) shape: (784, 10)\n",
      "Trained Bias (Hidden Layer) shape: (1, 10)\n",
      "Trained Weights (Output Layer) shape: (10, 10)\n",
      "Trained Bias (Output Layer) shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "no_of_rows, no_of_cols = X_train.shape\n",
    "\n",
    "# Initialize weights and biases for the first layer (hidden layer)\n",
    "hidden_layer_neurons = 10\n",
    "weights_hidden = np.random.randn(no_of_cols, hidden_layer_neurons)\n",
    "bias_hidden = np.random.rand(1, hidden_layer_neurons)\n",
    "\n",
    "# Initialize weights and biases for the output layer\n",
    "weights_output = np.zeros([hidden_layer_neurons, 10])  # Assuming 10 classes\n",
    "bias_output = np.random.rand(1, 10)\n",
    "\n",
    "learning_rate = 1\n",
    "num_epochs = 1000\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "target_one_hot = np.eye(10)[Y_train]\n",
    "\n",
    "# Forward Pass\n",
    "inputs = np.array(X_train)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass - Hidden Layer\n",
    "    hidden_layer_output = 1 / (1 + np.exp(-(np.dot(inputs, weights_hidden) + bias_hidden)))\n",
    "\n",
    "    # Forward Pass - Output Layer\n",
    "    logits = np.dot(hidden_layer_output, weights_output) + bias_output\n",
    "    exp_logits = np.exp(logits)\n",
    "    softmax_output = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = -np.sum(target_one_hot * np.log(softmax_output + 1e-8)) / no_of_rows\n",
    "\n",
    "    # Optional: Print the loss for monitoring convergence\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Cross-Entropy Loss: {loss}\")\n",
    "\n",
    "    # Backward Pass\n",
    "    d_logits = softmax_output - target_one_hot\n",
    "\n",
    "    # Backward Pass - Output Layer\n",
    "    d_weights_output = np.dot(hidden_layer_output.T, d_logits) / no_of_rows\n",
    "    d_bias_output = np.sum(d_logits, axis=0, keepdims=True) / no_of_rows\n",
    "\n",
    "    # Backward Pass - Hidden Layer\n",
    "    d_hidden_layer = np.dot(d_logits, weights_output.T) * hidden_layer_output * (1 - hidden_layer_output)\n",
    "    d_weights_hidden = np.dot(inputs.T, d_hidden_layer) / no_of_rows\n",
    "    d_bias_hidden = np.sum(d_hidden_layer, axis=0, keepdims=True) / no_of_rows\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_output -= learning_rate * d_weights_output\n",
    "    bias_output -= learning_rate * d_bias_output\n",
    "    weights_hidden -= learning_rate * d_weights_hidden\n",
    "    bias_hidden -= learning_rate * d_bias_hidden\n",
    "\n",
    "# After training, 'weights_hidden', 'bias_hidden', 'weights_output', and 'bias_output' will be adjusted for classification.\n",
    "print(\"Trained Weights (Hidden Layer) shape:\", weights_hidden.shape)\n",
    "print(\"Trained Bias (Hidden Layer) shape:\", bias_hidden.shape)\n",
    "print(\"Trained Weights (Output Layer) shape:\", weights_output.shape)\n",
    "print(\"Trained Bias (Output Layer) shape:\", bias_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25020\\1209649988.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  hidden_layer_output = 1 / (1 + np.exp(-(np.dot(X, weights_hidden) + bias_hidden)))\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass for Testing\n",
    "def predict(X, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    hidden_layer_output = 1 / (1 + np.exp(-(np.dot(X, weights_hidden) + bias_hidden)))\n",
    "    logits = np.dot(hidden_layer_output, weights_output) + bias_output\n",
    "    exp_logits = np.exp(logits)\n",
    "    softmax_output = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    return np.argmax(softmax_output, axis=1)\n",
    "\n",
    "# Convert target labels to one-hot encoding for test set\n",
    "target_one_hot_test = np.eye(10)[Y_test]\n",
    "\n",
    "# Use the trained weights and biases for prediction\n",
    "predictions = predict(X_test, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == Y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 3\n",
      "True Class: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25020\\4195618753.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  hidden_layer_output = 1 / (1 + np.exp(-(np.dot(input_vector, weights_hidden) + bias_hidden)))\n"
     ]
    }
   ],
   "source": [
    "def predict_class_with_hidden(input_vector, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    # Forward Pass - Hidden Layer\n",
    "    hidden_layer_output = 1 / (1 + np.exp(-(np.dot(input_vector, weights_hidden) + bias_hidden)))\n",
    "\n",
    "    # Forward Pass - Output Layer\n",
    "    logits = np.dot(hidden_layer_output, weights_output) + bias_output\n",
    "    softmax_output = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "\n",
    "    # Predicted Class\n",
    "    predicted_class = np.argmax(softmax_output, axis=1)[0]\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "input_example = X_test[1]  # Replace with the input vector you want to predict\n",
    "predicted_class_example = predict_class_with_hidden(input_example, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class_example}\")\n",
    "print(f\"True Class: {Y_test[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
