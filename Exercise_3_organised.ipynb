{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Question 1\n",
    "##################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# reading the CSV file\n",
    "df = pd.read_csv(r\"mnist_train.csv\")\n",
    " \n",
    "# displaying the contents of the CSV file\n",
    "x = df['label'].tolist()\n",
    "Y = np.array(x)\n",
    "# print(\"Y\",\"=\",Y)  # getting the output from the provided data \n",
    "c = df.drop(['label'], axis=1) #dropping the label axis from the main data and getting the remaining array as x\n",
    "X = (c.to_numpy())\n",
    "# print(\"X\",\"=\",X)\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    ############      Initializing all the parameters with the help of __init__() function.\n",
    "    def __init__(self, network, activation_function, derivative_activation_function, loss_func, loss_func_derivative,learning_rate, epoch, diff_cost, Ypred, init_weights, init_bias, optimizer, beta):\n",
    "        self.network = network\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative_function = derivative_activation_function\n",
    "        self.weights = init_weights\n",
    "        self.bias = init_bias\n",
    "        self.activation = []\n",
    "        self.last_layer_derivative = []\n",
    "        self.loss_func = loss_func\n",
    "        self.loss_func_derivative = loss_func_derivative\n",
    "        self.diff_cost = diff_cost\n",
    "        self.Ypred = Ypred\n",
    "        self.optimizer = optimizer\n",
    "        self.beta = beta\n",
    "        self.weight_error_list = []\n",
    "        self.bias_error_list = []\n",
    "        for i in range(len(self.network)):\n",
    "            self.weight_error_list.append([])\n",
    "            self.bias_error_list.append([])\n",
    "    \n",
    "####### Creating the forward pass algorithm.\n",
    "    def forward_pass(self,X):\n",
    "        self.activation = []\n",
    "        A = X\n",
    "        print(A.size)\n",
    "        self.activation.append(A)\n",
    "        for i in range(len(self.weights)-1):\n",
    "            Z = np.dot(A, self.weights[i]) + self.bias[i]\n",
    "            print(self.weights[i])\n",
    "            A = self.activation_function(Z) \n",
    "            self.activation.append(A)\n",
    "        Z = np.dot(self.activation[-1], self.weights[-1]) + self.bias[-1]\n",
    "        A = self.activation_function(Z)\n",
    "        self.activation.append(A)\n",
    "\n",
    "########## Creating the algorithm for gradient descent\n",
    "    def gradient_descent(self, last_weights, last_bias,weight_error, bias_error, index):\n",
    "        if self.optimizer == 'momentum':\n",
    "            n = int(len(self.weight_error_list[index]))\n",
    "            weight_velocity = 0\n",
    "            bias_velocity = 0\n",
    "            for i in range(0, len(self.weight_error_list[index])):\n",
    "                weight_velocity += (self.beta ** (n - i)) * self.weight_error_list[index][i]  \n",
    "                bias_velocity += (self.beta ** (n - i)) * self.bias_error_list[index][i]      \n",
    "            weight_velocity = (1 - self.beta)*weight_velocity\n",
    "            bias_velocity = (1 - self.beta)*bias_velocity\n",
    "            new_weights = last_weights - self.learning_rate * weight_velocity\n",
    "            new_bias = last_bias - self.learning_rate * bias_velocity  \n",
    "        elif self.optimizer == 'simple':\n",
    "            new_weights = last_weights - self.learning_rate * weight_error\n",
    "            new_bias = last_bias - self.learning_rate * bias_error\n",
    "        return new_weights, new_bias\n",
    "\n",
    "\n",
    "################ Creating the algorithm for the backward pass.\n",
    "    def backward_pass(self, X, Y):\n",
    "        next_layer_error = self.loss_func_derivative(Y, self.activation[-1]) \n",
    "\n",
    "        for i in reversed(range(0, (len(self.activation) - 1))):\n",
    "            #print(i, self.activation[i].shape)\n",
    "            out_fac = np.multiply(next_layer_error, self.activation_derivative_function(self.activation[i+1]))\n",
    "            weight_error = (self.activation[i].T) @ out_fac\n",
    "            bias_error = np.sum(out_fac, axis = 0)\n",
    "            #print(weight_error.shape)\n",
    "            next_layer_error =  out_fac @ self.weights[i].T\n",
    "            self.weight_error_list[i].append(weight_error)\n",
    "            self.bias_error_list[i].append(bias_error)\n",
    "            #print(i)\n",
    "            self.weights[i], self.bias[i] = self.gradient_descent(self.weights[i], self.bias[i], weight_error, bias_error, i)\n",
    "            #self.weights[i] -= self.learning_rate * weight_error\n",
    "            #self.bias[i] -= self.learning_rate * bias_error\n",
    "        \n",
    "\n",
    "############## Function for calculating the cost\n",
    "    def cost_calc(self, Ypred, Yactual):\n",
    "        return self.loss_func(Yactual, Ypred)\n",
    "\n",
    "\n",
    "    def predict(self, X_given):\n",
    "        self.forward_pass(X_given)\n",
    "        predict_list = []\n",
    "        for i in self.activation[-1]:\n",
    "            predict_list.append(np.argmax(i))\n",
    "        return predict_list\n",
    "\n",
    "    def flattening(self, Y):\n",
    "        a, b = Y.shape[0], self.network[-1]\n",
    "        new_Y = np.zeros((a,b))\n",
    "        for i in range(Y.shape[0]):\n",
    "            new_Y[i, Y[i]] = 1 \n",
    "\n",
    "        return new_Y\n",
    "\n",
    "        \n",
    "    def accuracy(self, X_test, Y_test):\n",
    "        Y_pred = np.array(self.predict(X_test))\n",
    "        correct_results = 0\n",
    "        for i in range(Y_test.shape[0]):\n",
    "            if Y_pred[i] == Y_test[i]:\n",
    "                correct_results += 1\n",
    "        return correct_results/Y_test.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### Algorithm for training means \n",
    "    def train(self, X, Y, bound):\n",
    "        i = 0\n",
    "        Y = self.flattening(Y)\n",
    "        \n",
    "        while i < self.epoch and self.diff_cost > bound:\n",
    "            prev_cost = self.cost_calc(self.Ypred, Y)\n",
    "            self.forward_pass(X)\n",
    "            self.backward_pass(X, Y)\n",
    "            self.Ypred = self.activation[-1]\n",
    "            i += 1\n",
    "            #number_of_times += 1\n",
    "            self.diff_cost = abs(self.cost_calc(self.Ypred, Y) - prev_cost)\n",
    "            print(\"Diff in Cost in the run \", self.diff_cost)\n",
    "        print(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid funstion\n",
    "def sigmoid(x):\n",
    "    return 1/(np.exp(-x)+1)    \n",
    "\n",
    "#derivative of sigmoid\n",
    "def sigmoid_gradient(x):\n",
    "    return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "\n",
    "# function for categorical cross entropy\n",
    "def categorical_cross_entropy(y, y_hat):\n",
    "    return -np.sum(y * np.log(y_hat + 1e-9))/y.shape[0]\n",
    "\n",
    " \n",
    "# function for categorical cross entropy\n",
    "def categorical_cross_entropy_prime(y, y_hat):\n",
    "    return -y/((y_hat+ 1e-9)*(y.shape[0]))\n",
    "\n",
    "# ReLu activation function and its gradient\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# gradient for relu function\n",
    "def relu_gradient(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000,)\n",
      "(12000, 784)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:int(X.shape[0]*0.8), :]\n",
    "Y_train = Y[:int(Y.shape[0]*0.8)]\n",
    "X_test = X[int(X.shape[0]*0.8):, :]\n",
    "Y_test = Y[int(Y.shape[0]*0.8):]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "bias = []\n",
    "network = [784, 10]\n",
    "\n",
    "def xavier_init(in_dim, out_dim):\n",
    "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "    return np.random.normal(0, xavier_stddev, (in_dim, out_dim))\n",
    "\n",
    "for i in range(len(network)-1):\n",
    "    weights.append(xavier_init(network[i], network[i+1]))\n",
    "\n",
    "for j in range(len(network)-1):\n",
    "    bias.append(xavier_init(1, network[j+1]))\n",
    "\n",
    "len(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([784, 10], relu, relu_gradient, categorical_cross_entropy,categorical_cross_entropy_prime, 0.1, 100, 100, np.ones((Y_train.shape[0], 10)), weights, bias, 'momentum',0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37632000\n",
      "Diff in Cost in the run  11.24624495778546\n",
      "37632000\n",
      "Diff in Cost in the run  9.524696739292802\n",
      "37632000\n",
      "Diff in Cost in the run  3.3091588239475724\n",
      "37632000\n",
      "Diff in Cost in the run  1.5361051392276523\n",
      "37632000\n",
      "Diff in Cost in the run  0.7532980716955833\n",
      "37632000\n",
      "Diff in Cost in the run  0.2998808674446609\n",
      "37632000\n",
      "Diff in Cost in the run  0.40432500000674754\n",
      "37632000\n",
      "Diff in Cost in the run  1.0290124860407142\n",
      "37632000\n",
      "Diff in Cost in the run  1.2398564122586446\n",
      "37632000\n",
      "Diff in Cost in the run  0.23743849555323582\n",
      "37632000\n",
      "Diff in Cost in the run  0.11551533852490081\n",
      "37632000\n",
      "Diff in Cost in the run  0.08542626318694069\n",
      "37632000\n",
      "Diff in Cost in the run  0.07010191121815712\n",
      "37632000\n",
      "Diff in Cost in the run  0.05916317027118634\n",
      "37632000\n",
      "Diff in Cost in the run  0.05079318365151764\n",
      "37632000\n",
      "Diff in Cost in the run  0.044140813924285816\n",
      "37632000\n",
      "Diff in Cost in the run  0.03871964003141759\n",
      "37632000\n",
      "Diff in Cost in the run  0.034222036851815574\n",
      "37632000\n",
      "Diff in Cost in the run  0.03043991159743875\n",
      "37632000\n",
      "Diff in Cost in the run  0.02722555677779237\n",
      "37632000\n",
      "Diff in Cost in the run  0.02447026375062844\n",
      "37632000\n",
      "Diff in Cost in the run  0.022091735297073534\n",
      "37632000\n",
      "Diff in Cost in the run  0.02002624015859311\n",
      "37632000\n",
      "Diff in Cost in the run  0.01822349623579278\n",
      "37632000\n",
      "Diff in Cost in the run  0.016643208477828075\n",
      "37632000\n",
      "Diff in Cost in the run  0.015252654099056073\n",
      "37632000\n",
      "Diff in Cost in the run  0.014024954702681391\n",
      "37632000\n",
      "Diff in Cost in the run  0.012937812640874569\n",
      "37632000\n",
      "Diff in Cost in the run  0.011972569266889721\n",
      "37632000\n",
      "Diff in Cost in the run  0.01111349138066231\n",
      "37632000\n",
      "Diff in Cost in the run  0.01034722260655574\n",
      "37632000\n",
      "Diff in Cost in the run  0.009662356035030406\n",
      "37632000\n",
      "Diff in Cost in the run  0.009049097388796667\n",
      "37632000\n",
      "Diff in Cost in the run  0.008498996696197914\n",
      "37632000\n",
      "Diff in Cost in the run  0.00800473245468325\n",
      "37632000\n",
      "Diff in Cost in the run  0.007559936467028017\n",
      "37632000\n",
      "Diff in Cost in the run  0.007159050520876953\n",
      "37632000\n",
      "Diff in Cost in the run  0.0067972082365557185\n",
      "37632000\n",
      "Diff in Cost in the run  0.006470136983740993\n",
      "37632000\n",
      "Diff in Cost in the run  0.006174075932052325\n",
      "37632000\n",
      "Diff in Cost in the run  0.005905707171921293\n",
      "37632000\n",
      "Diff in Cost in the run  0.005662097499770091\n",
      "37632000\n",
      "Diff in Cost in the run  0.0054406489632246036\n",
      "37632000\n",
      "Diff in Cost in the run  0.005239056647552864\n",
      "37632000\n",
      "Diff in Cost in the run  0.005055272483882156\n",
      "37632000\n",
      "Diff in Cost in the run  0.004887474092870825\n",
      "37632000\n",
      "Diff in Cost in the run  0.0047340378617724\n",
      "37632000\n",
      "Diff in Cost in the run  0.004593515598005915\n",
      "37632000\n",
      "Diff in Cost in the run  0.004464614218789187\n",
      "37632000\n",
      "Diff in Cost in the run  0.004346178029205561\n",
      "37632000\n",
      "Diff in Cost in the run  0.004237173216362855\n",
      "37632000\n",
      "Diff in Cost in the run  0.004136674248329086\n",
      "37632000\n",
      "Diff in Cost in the run  0.00404385191603307\n",
      "37632000\n",
      "Diff in Cost in the run  0.003957962797327852\n",
      "37632000\n",
      "Diff in Cost in the run  0.003878339956017385\n",
      "37632000\n",
      "Diff in Cost in the run  0.00380438471623723\n",
      "37632000\n",
      "Diff in Cost in the run  0.0037355593761105865\n",
      "37632000\n",
      "Diff in Cost in the run  0.0036713807437696033\n",
      "37632000\n",
      "Diff in Cost in the run  0.0036114143948866584\n",
      "37632000\n",
      "Diff in Cost in the run  0.003555269565143071\n",
      "37632000\n",
      "Diff in Cost in the run  0.0035025946019464627\n",
      "37632000\n",
      "Diff in Cost in the run  0.0034530729102018043\n",
      "37632000\n",
      "Diff in Cost in the run  0.003406419334855748\n",
      "37632000\n",
      "Diff in Cost in the run  0.003362376930536648\n",
      "37632000\n",
      "Diff in Cost in the run  0.003320714074483533\n",
      "37632000\n",
      "Diff in Cost in the run  0.0032812218844764374\n",
      "37632000\n",
      "Diff in Cost in the run  0.0032437119080430676\n",
      "37632000\n",
      "Diff in Cost in the run  0.0032080140531682844\n",
      "37632000\n",
      "Diff in Cost in the run  0.0031739747342935942\n",
      "37632000\n",
      "Diff in Cost in the run  0.0031414552103843363\n",
      "37632000\n",
      "Diff in Cost in the run  0.003110330094349578\n",
      "37632000\n",
      "Diff in Cost in the run  0.0030804860158841763\n",
      "37632000\n",
      "Diff in Cost in the run  0.003051820421090312\n",
      "37632000\n",
      "Diff in Cost in the run  0.0030242404948559454\n",
      "37632000\n",
      "Diff in Cost in the run  0.0029976621928842206\n",
      "37632000\n",
      "Diff in Cost in the run  0.0029720093720495555\n",
      "37632000\n",
      "Diff in Cost in the run  0.002947213008894778\n",
      "37632000\n",
      "Diff in Cost in the run  0.0029232104970233763\n",
      "37632000\n",
      "Diff in Cost in the run  0.0028999450154660877\n",
      "37632000\n",
      "Diff in Cost in the run  0.0028773649605238205\n",
      "37632000\n",
      "Diff in Cost in the run  0.0028554234346795937\n",
      "37632000\n",
      "Diff in Cost in the run  0.00283407778680278\n",
      "37632000\n",
      "Diff in Cost in the run  0.002813289198336122\n",
      "37632000\n",
      "Diff in Cost in the run  0.0027930223107528462\n",
      "37632000\n",
      "Diff in Cost in the run  0.0027732448903083906\n",
      "37632000\n",
      "Diff in Cost in the run  0.0027539275260046736\n",
      "37632000\n",
      "Diff in Cost in the run  0.0027350433577471023\n",
      "37632000\n",
      "Diff in Cost in the run  0.002716567831260619\n",
      "37632000\n",
      "Diff in Cost in the run  0.0026984784775638815\n",
      "37632000\n",
      "Diff in Cost in the run  0.0026807547139693355\n",
      "37632000\n",
      "Diff in Cost in the run  0.00266337766496072\n",
      "37632000\n",
      "Diff in Cost in the run  0.0026463300005996615\n",
      "37632000\n",
      "Diff in Cost in the run  0.002629595790903494\n",
      "37632000\n",
      "Diff in Cost in the run  0.0026131603744676823\n",
      "37632000\n",
      "Diff in Cost in the run  0.0025970102400556527\n",
      "37632000\n",
      "Diff in Cost in the run  0.002581132919704743\n",
      "37632000\n",
      "Diff in Cost in the run  0.002565516892317987\n",
      "37632000\n",
      "Diff in Cost in the run  0.0025501514966421723\n",
      "37632000\n",
      "Diff in Cost in the run  0.002535026852708455\n",
      "37632000\n",
      "Diff in Cost in the run  0.0025201337909503962\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "model.epoch = 100\n",
    "model.train(X_train, Y_train, 1e-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#  Question 2\n",
    "###############################\n",
    "\n",
    "\n",
    "def Question2(X, Y, init_weight, init_bias, learning_rate):\n",
    "    model = MLP([784,10], relu, relu_gradient, categorical_cross_entropy,categorical_cross_entropy_prime, learning_rate, 1, 100,np.ones((Y_train.shape[0], 10)), init_weight, init_bias, optimizer = 'simple', beta = 0)\n",
    "    return model.weights, model.bias\n",
    "# this function returns the updated weights and biases\n",
    "Question2(X,Y,weights, bias, 0.001)\n",
    "weights, bias = Question2(X,Y,weights, bias, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_data = pd.read_csv(\"mnist_train.csv\")\n",
    "\n",
    "# Assuming the labels are in the first column and pixel values start from the second column\n",
    "X = mnist_data.iloc[:, 1:]\n",
    "Y = mnist_data.iloc[:, 0]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# By default, the test_size is set to 0.25, meaning 25% of the data will be used for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: [0 0 0 0 0 0 0 0 0 0]\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_2148\\2528251222.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  softmax_output = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_2148\\2528251222.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax_output = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "def predict_class(input_vector):\n",
    "    input_vector = np.array(input_vector).reshape(1, -1)\n",
    "    logits = np.dot(input_vector, weights) + bias\n",
    "    softmax_output = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    predicted_class = np.argmax(softmax_output, axis=1)[0]\n",
    "    return predicted_class\n",
    "\n",
    "# # Example usage\n",
    "input_example = X_test[1004]  # Replace with the input vector you want to predict\n",
    "predicted_class_example = predict_class(input_example)\n",
    "print(f\"Predicted Class: {predicted_class_example}\")\n",
    "print(Y_test[1004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (45000, 784)\n",
      "X_test shape: (15000, 784)\n",
      "y_train shape: (45000,)\n",
      "y_test shape: (15000,)\n",
      "(15000,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLP.__init__() missing 13 required positional arguments: 'network', 'activation_function', 'derivative_activation_function', 'loss_func', 'loss_func_derivative', 'learning_rate', 'epoch', 'diff_cost', 'Ypred', 'init_weights', 'init_bias', 'optimizer', and 'beta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"X_test is : \" ,X_test)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# print(X_test.shape)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# print(\"Y_test is : \" , Y_test)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy(X_test,Y_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: MLP.__init__() missing 13 required positional arguments: 'network', 'activation_function', 'derivative_activation_function', 'loss_func', 'loss_func_derivative', 'learning_rate', 'epoch', 'diff_cost', 'Ypred', 'init_weights', 'init_bias', 'optimizer', and 'beta'"
     ]
    }
   ],
   "source": [
    "#### Splitting the data into test and train datasets\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_data = pd.read_csv(\"mnist_train.csv\")\n",
    "\n",
    "# Assuming the labels are in the first column and pixel values start from the second column\n",
    "X = mnist_data.iloc[:, 1:]\n",
    "Y = mnist_data.iloc[:, 0]\n",
    "# print(X)\n",
    "# print(X.shape)\n",
    "# print(Y)\n",
    "# print(Y.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# By default, the test_size is set to 0.25, meaning 25% of the data will be used for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", Y_train.shape)\n",
    "print(\"y_test shape:\", Y_test.shape)\n",
    "# print(\"X_test is : \" ,X_test)\n",
    "# print(X_test.shape)\n",
    "# print(\"Y_test is : \" , Y_test)\n",
    "print(Y_test.shape)\n",
    "\n",
    "\n",
    "MLP().accuracy(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.00433276, -0.04921942,  0.07032036, ...,  0.04543782,\n",
       "           0.00232675, -0.0492368 ],\n",
       "         [-0.05681664, -0.00558825,  0.09881505, ...,  0.02941765,\n",
       "           0.01717794,  0.01828841],\n",
       "         [-0.00957505, -0.01864338, -0.08053478, ...,  0.02758056,\n",
       "           0.06751622, -0.05917661],\n",
       "         ...,\n",
       "         [ 0.08745037,  0.00737087,  0.05731651, ..., -0.13073172,\n",
       "          -0.0019095 , -0.05602664],\n",
       "         [-0.01890735, -0.07924546, -0.05136649, ..., -0.07931404,\n",
       "           0.00901902, -0.05785499],\n",
       "         [ 0.05864962,  0.08517096,  0.02903769, ...,  0.06477124,\n",
       "           0.02560436,  0.02573085]])],\n",
       " [array([[ 0.27980491, -0.23114314,  0.3057736 , -0.46551313,  0.64912974,\n",
       "          -0.38258846,  0.03504389, -0.31666674,  0.37115068,  1.27388835]])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################\n",
    "# Question 3\n",
    "#####################\n",
    "\n",
    "def Question3(X, Y, init_weight, init_bias, learning_rate):\n",
    "    network_list = []\n",
    "    for i in range(len(init_weight)):\n",
    "        network_list.append(init_weight[i].shape[0])\n",
    "    network_list.append(init_weight[-1].shape[1])\n",
    "    model = MLP(network_list, relu, relu_gradient, categorical_cross_entropy,categorical_cross_entropy_prime, learning_rate, 1, 100,np.ones((Y_train.shape[0], 10)), init_weight, init_bias, optimizer = 'simple', beta = 0)\n",
    "    return model.weights, model.bias\n",
    "\n",
    "\n",
    "Question3(X,Y, weights, bias, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.00433276, -0.04921942,  0.07032036, ...,  0.04543782,\n",
       "           0.00232675, -0.0492368 ],\n",
       "         [-0.05681664, -0.00558825,  0.09881505, ...,  0.02941765,\n",
       "           0.01717794,  0.01828841],\n",
       "         [-0.00957505, -0.01864338, -0.08053478, ...,  0.02758056,\n",
       "           0.06751622, -0.05917661],\n",
       "         ...,\n",
       "         [ 0.08745037,  0.00737087,  0.05731651, ..., -0.13073172,\n",
       "          -0.0019095 , -0.05602664],\n",
       "         [-0.01890735, -0.07924546, -0.05136649, ..., -0.07931404,\n",
       "           0.00901902, -0.05785499],\n",
       "         [ 0.05864962,  0.08517096,  0.02903769, ...,  0.06477124,\n",
       "           0.02560436,  0.02573085]])],\n",
       " [array([[ 0.27980491, -0.23114314,  0.3057736 , -0.46551313,  0.64912974,\n",
       "          -0.38258846,  0.03504389, -0.31666674,  0.37115068,  1.27388835]])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################\n",
    "# Question 4\n",
    "###################### \n",
    "\n",
    "def Question4(X, Y, init_weight, init_bias, learning_rate):\n",
    "    network_list = []\n",
    "    for i in range(len(init_weight)):\n",
    "        network_list.append(init_weight[i].shape[0])\n",
    "    network_list.append(init_weight[-1].shape[1])\n",
    "    model = MLP(network_list, sigmoid, sigmoid_gradient, categorical_cross_entropy,categorical_cross_entropy_prime, learning_rate, 1, 100,np.ones((Y_train.shape[0], 10)), init_weight, init_bias, optimizer = 'simple', beta = 0)\n",
    "    return model.weights, model.bias\n",
    "\n",
    "Question4(X,Y, weights, bias, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.00433276, -0.04921942,  0.07032036, ...,  0.04543782,\n",
       "           0.00232675, -0.0492368 ],\n",
       "         [-0.05681664, -0.00558825,  0.09881505, ...,  0.02941765,\n",
       "           0.01717794,  0.01828841],\n",
       "         [-0.00957505, -0.01864338, -0.08053478, ...,  0.02758056,\n",
       "           0.06751622, -0.05917661],\n",
       "         ...,\n",
       "         [ 0.08745037,  0.00737087,  0.05731651, ..., -0.13073172,\n",
       "          -0.0019095 , -0.05602664],\n",
       "         [-0.01890735, -0.07924546, -0.05136649, ..., -0.07931404,\n",
       "           0.00901902, -0.05785499],\n",
       "         [ 0.05864962,  0.08517096,  0.02903769, ...,  0.06477124,\n",
       "           0.02560436,  0.02573085]])],\n",
       " [array([[ 0.27980491, -0.23114314,  0.3057736 , -0.46551313,  0.64912974,\n",
       "          -0.38258846,  0.03504389, -0.31666674,  0.37115068,  1.27388835]])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "# Question 5\n",
    "##################\n",
    "\n",
    "def Question5(X, Y, init_weight, init_bias, learning_rate, activation_function, activation_function_derivative):\n",
    "    network_list = []\n",
    "    for i in range(len(init_weight)):\n",
    "        network_list.append(init_weight[i].shape[0])\n",
    "    network_list.append(init_weight[-1].shape[1])\n",
    "    model = MLP(network_list, activation_function, activation_function_derivative, categorical_cross_entropy,categorical_cross_entropy_prime, learning_rate, 1, 100,np.ones((Y_train.shape[0], 10)),init_weight, init_bias, optimizer = 'simple', beta = 0)\n",
    "    return model.weights, model.bias\n",
    "\n",
    "Question5(X,Y, weights, bias, 0.001, relu, relu_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[ 0.00433276, -0.04921942,  0.07032036, ...,  0.04543782,\n",
       "           0.00232675, -0.0492368 ],\n",
       "         [-0.05681664, -0.00558825,  0.09881505, ...,  0.02941765,\n",
       "           0.01717794,  0.01828841],\n",
       "         [-0.00957505, -0.01864338, -0.08053478, ...,  0.02758056,\n",
       "           0.06751622, -0.05917661],\n",
       "         ...,\n",
       "         [ 0.08745037,  0.00737087,  0.05731651, ..., -0.13073172,\n",
       "          -0.0019095 , -0.05602664],\n",
       "         [-0.01890735, -0.07924546, -0.05136649, ..., -0.07931404,\n",
       "           0.00901902, -0.05785499],\n",
       "         [ 0.05864962,  0.08517096,  0.02903769, ...,  0.06477124,\n",
       "           0.02560436,  0.02573085]])],\n",
       " [array([[ 0.27980491, -0.23114314,  0.3057736 , -0.46551313,  0.64912974,\n",
       "          -0.38258846,  0.03504389, -0.31666674,  0.37115068,  1.27388835]])])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "# Question 6\n",
    "########################\n",
    "\n",
    "\n",
    "def Question6(X, Y, init_weight, init_bias, learning_rate, activation_function, activation_function_derivative, optimizer):\n",
    "    network_list = []\n",
    "    for i in range(len(init_weight)):\n",
    "        network_list.append(init_weight[i].shape[0])\n",
    "    network_list.append(init_weight[-1].shape[1])\n",
    "    print(network)\n",
    "    model = MLP(network_list, activation_function, activation_function_derivative, categorical_cross_entropy,categorical_cross_entropy_prime, learning_rate, 1, 100,np.ones((Y_train.shape[0], 10)),init_weight, init_bias, optimizer = optimizer, beta = 0.9)\n",
    "    return model.weights, model.bias\n",
    "\n",
    "Question6(X,Y, weights, bias, 0.001, sigmoid, sigmoid_gradient, 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
